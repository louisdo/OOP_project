train_data_path: "./data/folder_16052020_v2/data.csv" # path to .csv file that contains training data, 
                                         # the .csv file should contain the 'source' and the 'dest column'

raw_data_path: "./data/Vnexpress.CLASSIFIED.VNINDEX.txt"

test_data_path: ""                       # path to .csv file that contains testing data, 
                                         # the .csv file should contain the 'source' and the 'dest column'

ckpt_folder: "data/folder_16052020_v2/ckpt/testing"         # the path to the folder where the checkpoint will be saved,
                                         # the folder should include 2 files: 'best_checkpoint.pth.tar' and 'best_config.json'

vocab_path: "data/folder_16052020_v2/vocab/"       # the path to the folder where the vocab is saved, 
                                         # the folder should include 2 files: 'index2word.json' and 'word2index.json'

resume_path: ""                          # the path to the resume checkpoint file

device: "cuda"                            # using which device to train, should be either "gpu" or "cpu"

max_len: 60                              # max training sentence length

batch_size: 4                            # self-explanatory

d_model: 300                             # dimensionality of the main model Transformer, for more information
                                         # please refer to 'https://arxiv.org/abs/1706.03762'

nhead: 6                                 # number of attention heads for multi-head attention operation within
                                         # the main model, for more information please refer to 'https://arxiv.org/abs/1706.03762'

num_layers: 3                            # number of layers of both decoder and encoder block

dropout: 0.1                             # dropout for training model

epochs: 5                              # number of training epochs

lr: 0.0001                               # learning rate

lr_decay: 0.1                            # learning rate decay rate

step_size: 2                             # step size for learning rate scheduler